# -*- coding: utf-8 -*-
"""Group21:Final_Midsem_project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UyFmBWIXoraI1gco-Ph2q7Il_VLV25VR

# Group21: Intro to AI mini project

# Authors: Priscile Nkenmeza Nzonbi, Jackline Mpaye

**Players overall Rating predictions**
"""

from sklearn.impute import SimpleImputer
from sklearn.preprocessing import LabelEncoder
import pandas as pd
import numpy as np
from google.colab import drive
drive.mount('/content/drive')

import sklearn
sklearn.__version__

"""Loading the datasets"""

data_21 = pd.read_csv('/content/drive/My Drive/Colab Notebooks/players_21.csv')

data_22 = pd.read_csv('/content/drive/My Drive/Colab Notebooks/players_22.csv')

"""# 1. Data Preparation & Feature Extraction

# Processing the 2021 dataset

Exploring the datasets to have an overview of the data
"""

data_21

data_21.head()

data_21.info()

"""Because the dataset has a lot of colums, we are going to split the dataset into subsets inorder for us to see which columns are numeric, which are non-numeric and deal with them accordingly. At the end of it all, we are going to join these sub datasets together and use them in our model."""

numeric_data_21= data_21.select_dtypes(include=['int64','float64'])
categorical_data_21=data_21.select_dtypes(exclude=['int64','float64'])

numeric_data_21.info()
categorical_data_21.info()

"""We drop all columns with more than 30% missing values"""

numeric_data_21 = numeric_data_21.dropna(axis=1, how='all')

numeric_data_21.info()

categorical_data_21 = categorical_data_21.dropna(axis=1, how='all')

categorical_data_21.info()

N_thresh = 0.3*len(numeric_data_21)
N_thresh

C_thresh = 0.3*len(categorical_data_21)
C_thresh

numeric_data_21 =numeric_data_21.dropna(thresh = N_thresh, axis =1)
categorical_data_21 = categorical_data_21.dropna(thresh =C_thresh, axis =1)

numeric_data_21.info()

categorical_data_21.info()

column_names = [
    'player_url', 'short_name', 'long_name', 'player_positions', 'dob',
    'club_name', 'league_name', 'club_position', 'club_joined',
    'nationality_name', 'preferred_foot', 'work_rate', 'body_type',
    'real_face', 'player_traits', 'ls', 'st', 'rs', 'lw', 'lf', 'cf',
    'rf', 'rw', 'lam', 'cam', 'ram', 'lm', 'lcm', 'cm', 'rcm', 'rm',
    'lwb', 'ldm', 'cdm', 'rdm', 'rwb', 'lb', 'lcb', 'cb', 'rcb', 'rb',
    'gk', 'player_face_url', 'club_logo_url', 'club_flag_url', 'nation_flag_url']

for column in column_names:
  categorical_data_21[column], _ = pd.factorize(categorical_data_21[column])

categorical_data_21

combined_data = pd.concat([numeric_data_21, categorical_data_21], axis=1)
combined_data.info()

"""Imputing missing values in the new numeric-only dataset"""

from sklearn.impute import SimpleImputer
imputer = SimpleImputer(strategy='mean')
combined_data = pd.DataFrame(imputer.fit_transform(combined_data), columns = combined_data.columns)
combined_data.info(verbose=True, null_counts = True)

"""# 2. Creating feature subsets that show maximum correlation with the dependent variable.

**Fining important features in the numerical dataset by correlation**
"""

corr = combined_data.corr()['overall'].sort_values(ascending=False)
corr

columns = corr[(corr) > 0.5].index
combined_data = combined_data[columns]
combined_data.info()

Y = combined_data['overall']
X = combined_data.drop(columns=['overall'])
X.info(verbose=True)

"""#### Scaling"""

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

X_scaled

X_scaled = pd.DataFrame(X_scaled, columns=X.columns)
X_scaled.info()

"""# 3. Create and train a suitable machine learning model with cross-validation that can predict a player's rating.
Using 2021 dataset

### Creating The Models
"""

from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from math import sqrt
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.ensemble import RandomForestRegressor
import xgboost as xgb


random_forest_model = RandomForestRegressor(random_state = 42)
xgboost_model = xgb.XGBRegressor(random_state = 42)
gradient_boost_model = GradientBoostingRegressor(random_state = 42)

# Split the data
Xtrain, Xtest, Ytrain, Ytest = train_test_split(X_scaled, Y, test_size=0.3, random_state=42)

"""#### Initial Model Training to Measure the models' performance"""

random_forest_model.fit(Xtrain, Ytrain)

xgboost_model.fit(Xtrain, Ytrain)

gradient_boost_model.fit(Xtrain, Ytrain)

"""We make predictions on the test data using bthe three models"""

random_pred = np.round(random_forest_model.predict(Xtest))
xg_pred= np.round(xgboost_model.predict(Xtest))
gradient_pred = np.round(gradient_boost_model.predict(Xtest))

results_comparision = pd.DataFrame({"Actual Rating" : Ytest, "Random Forest Prediction" : random_pred, "XGBoost Prediction" : xg_pred, "Gradient Boosting Prediction" : gradient_pred})

print(results_comparision)

"""We evaluate the models by calculating MAE and RMSE"""

#random forest
random_mae = mean_absolute_error(Ytest, random_pred)
random_rmse = sqrt(mean_squared_error(Ytest, random_pred))
random_r2 = r2_score(Ytest, random_pred)

#Xgboost
Xgboost_mae = mean_absolute_error(Ytest, xg_pred)
Xgboost_rmse = sqrt(mean_squared_error(Ytest, xg_pred))
xgboost_r2 = r2_score(Ytest, xg_pred)

# Gradient Boosting Regressor
gb_mae = mean_absolute_error(Ytest, gradient_pred)
gb_rmse = sqrt(mean_squared_error(Ytest, gradient_pred))
gb_r2 = r2_score(Ytest, gradient_pred)

#Mean absolute error for the models
print(f"random forest MAE: {random_mae}")
print(f"Xgboost forest MAE: {Xgboost_mae}")
print(f"Gradient Boosting MAE: {gb_mae}\n")

#Root mean square error for the models

print(f"random forest RMSE: {random_rmse}")
print(f"Xgboost forest RMSE: {Xgboost_rmse}")
print(f"Gradient Boosting RMSE: {gb_rmse}\n")

# R_2 score values
print(f"R_2 score for Random Forest: {random_r2}")
print(f"R_2 score for XGBoost: {xgboost_r2}")
print(f"R_2 score for Gradient Boosting Regressor: {gb_r2}")

"""### We hypertune our parameters using Grid Search with Cross validation"""

from sklearn.model_selection import GridSearchCV, KFold

random_forest_grid_parameters = {
    'n_estimators': [200, 300, 400],
    'min_samples_split': [2, 5, 10],
    'max_depth': [10, 20, 30]
}

xg_grid_parameters = {
    'n_estimators': [200, 300, 400],
    'learning_rate': [0.01, 0.1, 0.2],
    'max_depth': [10, 20, 30]
}

gradient_grid_parameter = {
    'n_estimators': [200, 300, 400],
    'learning_rate': [0.01, 0.1, 0.2],
    'max_depth': [3, 15, 30]
}

cv = KFold(n_splits = 5, shuffle = True, random_state = 42)

random_grid_search = GridSearchCV(estimator=random_forest_model, param_grid=random_forest_grid_parameters, scoring='neg_mean_squared_error', cv=cv)
random_grid_search.fit(Xtrain, Ytrain)

xg_grid_search = GridSearchCV(estimator=xgboost_model, param_grid=xg_grid_parameters, scoring='neg_mean_squared_error', cv=cv)
xg_grid_search.fit(Xtrain, Ytrain)

gradient_grid_search = GridSearchCV(estimator=gradient_boost_model, param_grid=gradient_grid_parameter, scoring='neg_mean_squared_error', cv=cv)
gradient_grid_search .fit(Xtrain, Ytrain)

random_forest_best_params = random_grid_search.best_params_
print("Best Hyperparameters for random forest:" ,random_forest_best_params)

xg_best_params = xg_grid_search.best_params_
print("Best Hyperparameters for xgboost:" ,xg_best_params)

gradient_best_params = gradient_grid_search.best_params_
print("Best Hyperparameters for gradient_boost:" ,gradient_best_params)

"""Re-initialising the models with their corresponding best hyperparameters"""

random_forest_model.set_params(**random_forest_best_params)
xgboost_model.set_params(**xg_best_params)
gradient_boost_model.set_params(**gradient_best_params)

"""**Performing cross validation on our models**"""

from sklearn.model_selection import cross_val_score

# Perform k-fold cross-validation for our models
random_forest_cv_scores = cross_val_score(random_forest_model, Xtrain, Ytrain, cv=cv, scoring='neg_mean_squared_error')
random_forest_rmse = sqrt(-random_forest_cv_scores.mean())

xgboost_cv_scores = cross_val_score(xgboost_model, Xtrain, Ytrain, cv=cv, scoring='neg_mean_squared_error')
xgboost_rmse = sqrt(-xgboost_cv_scores.mean())

gradient_boost_cv_scores = cross_val_score(gradient_boost_model, Xtrain, Ytrain, cv=cv, scoring='neg_mean_squared_error')
gradient_boost_rmse = sqrt(-gradient_boost_cv_scores.mean())

# Print the cross-validated RMSE for each model
print("Cross-Validated RMSE for Random Forest:", random_forest_rmse)
print("Cross-Validated RMSE for XGBoost:", xgboost_rmse)
print("Cross-Validated RMSE for Gradient Boosting:", gradient_boost_rmse)

"""# Retraining our models with Best Hyperparameters"""

# Best hyperparameters obtained from grid search
random_forest_model.fit(Xtrain, Ytrain)

# Retrain XGBoost model with best hyperparameters
xgboost_model.fit(Xtrain, Ytrain)

# Retrain Gradient Boosting Regressor model with best hyperparameters
gradient_boost_model.fit(Xtrain, Ytrain)

"""Combining all 3 models with Stacking Regressor"""

from sklearn.ensemble import StackingRegressor
from sklearn.linear_model import LinearRegression

stacking_regressor_model = StackingRegressor(estimators = [
    ('random_forest', random_forest_model),
    ('xgboost', xgboost_model),
    ('gradient_boost', gradient_boost_model)
], final_estimator = LinearRegression())

stacking_regressor_model.fit(Xtrain, Ytrain)

final_random_pred = np.round(random_forest_model.predict(Xtest))
final_xg_pred= np.round(xgboost_model.predict(Xtest))
final_gradient_pred = np.round(gradient_boost_model.predict(Xtest))
final_stacking_pred = np.round(stacking_regressor_model.predict(Xtest))

results_comparision = pd.DataFrame({"Actual Rating" : Ytest, "Random Forest Prediction" : final_random_pred, "XGBoost Prediction" : final_xg_pred, "Gradient Boosting Prediction" : final_gradient_pred, "Stacking Regressor Prediction" : final_stacking_pred})
print(results_comparision)

"""Evaluating the best trained model performance"""

from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from math import sqrt

random_forest_mae = mean_absolute_error(Ytest, final_random_pred)
random_forest_rmse = sqrt(mean_squared_error(Ytest, final_random_pred))
random_forest_r2 = r2_score(Ytest, final_random_pred)

xgboost_mae = mean_absolute_error(Ytest, final_xg_pred)
xgboost_rmse = sqrt(mean_squared_error(Ytest, final_xg_pred))
xgboost_r2 = r2_score(Ytest, final_xg_pred)


gradient_mae = mean_absolute_error(Ytest, final_gradient_pred)
gradient_rmse = sqrt(mean_squared_error(Ytest, final_gradient_pred))
gradient_r2 = r2_score(Ytest, final_gradient_pred)

stacking_mae = mean_absolute_error(Ytest, final_stacking_pred)
stacking_rmse = sqrt(mean_squared_error(Ytest, final_stacking_pred))
stacking_r2 = r2_score(Ytest, final_stacking_pred)

print("Random Forest - Mean Absolute Error (MAE):", random_forest_mae)
print("Random Forest - Root Mean Squared Error (RMSE):", random_forest_rmse)
print("Random Forest - R-squared (R2) Score:", random_forest_r2, "\n")

print("xgboost - Mean Absolute Error (MAE):", xgboost_mae)
print("xgboost - Root Mean Squared Error (RMSE):", xgboost_rmse)
print("xgboost - R-squared (R2) Score:", xgboost_r2, "\n")

print("Gradient Boosting - Mean Absolute Error (MAE):", gradient_mae)
print("Gradient Boosting - Root Mean Squared Error (RMSE):", gradient_rmse)
print("Gradient Boosting - R-squared (R2) Score:", gradient_r2)

print("Stacking Regressor - Mean Absolute Error (MAE):", stacking_mae)
print("Stacking Regressor - Root Mean Squared Error (RMSE):", stacking_rmse)
print("Stacking Regressor - R-squared (R2) Score:", stacking_r2)

"""##### Exporting the best model"""

import pickle

best_model = stacking_regressor_model

# Saving the trained model to a file
with open('best_model.pkl', 'wb') as file:
    pickle.dump(best_model, file)

with open('scaler.pkl', 'wb') as file:
    pickle.dump(scaler, file)

"""# DATASET 2022

# Processing the 2022 dataset

Exploring the datasets to have an overview of the data
"""

data_22

data_22.head()

data_22.info()

numeric_data_22= data_22.select_dtypes(include=['int64','float64'])
categorical_data_22=data_22.select_dtypes(exclude=['int64','float64'])

numeric_data_22.info()
categorical_data_22.info()

"""We drop all columns with more than 30% missing values"""

numeric_data_22 = numeric_data_22.dropna(axis=1, how='all')

numeric_data_22.info()

categorical_data_22 = categorical_data_22.dropna(axis=1, how='all')

categorical_data_22.info()

N_thresh = 0.3*len(numeric_data_22)
N_thresh

C_thresh = 0.3*len(categorical_data_22)
C_thresh

numeric_data_22 =numeric_data_22.dropna(thresh = N_thresh, axis =1)
categorical_data_22 = categorical_data_22.dropna(thresh =C_thresh, axis =1)

numeric_data_22.info()

categorical_data_22.info()

column_names = [
    'player_url', 'short_name', 'long_name', 'player_positions', 'dob',
    'club_name', 'league_name', 'club_position', 'club_joined',
    'nationality_name', 'preferred_foot', 'work_rate', 'body_type',
    'real_face', 'player_traits', 'ls', 'st', 'rs', 'lw', 'lf', 'cf',
    'rf', 'rw', 'lam', 'cam', 'ram', 'lm', 'lcm', 'cm', 'rcm', 'rm',
    'lwb', 'ldm', 'cdm', 'rdm', 'rwb', 'lb', 'lcb', 'cb', 'rcb', 'rb',
    'gk', 'player_face_url', 'club_logo_url', 'club_flag_url', 'nation_flag_url']

for column in column_names:
  categorical_data_22[column], _ = pd.factorize(categorical_data_22[column])

categorical_data_22

combined_data2 = pd.concat([numeric_data_22, categorical_data_22], axis=1)
combined_data2.info()

"""Imputing missing values in the new numeric-only dataset"""

from sklearn.impute import SimpleImputer
imputer = SimpleImputer(strategy='mean')
combined_data2 = pd.DataFrame(imputer.fit_transform(combined_data2), columns = combined_data2.columns)
combined_data2.info(verbose=True, null_counts = True)

"""# 2. Creating feature subsets that show maximum correlation with the dependent variable.

**Fining important features in the numerical dataset by correlation**
"""

combined_data2 = combined_data2[combined_data.columns.tolist()]
combined_data2.info()

Y2 = combined_data2['overall']
X2 = combined_data2.drop(columns=['overall'])
X2.info(verbose=True)

"""#### Scaling"""

temp = pickle.load('scaler.pkl', 'rb')
X2_scaled = temp.transform(X2)


X2_scaled= pd.DataFrame(X2_scaled, columns=X2.columns)
X2_scaled.head()

"""# Processing the 2022 dataset

Loading our saved model
"""

import pickle

filename = 'best_model.pkl'

with open(filename, 'rb') as file:
    loaded_model = pickle.load(file)

"""Predincting 2022 rating with the model trained from the 2021 data"""

pred_2022 = loaded_model.predict(X2)

"""Making our prediction

Evaluating the model with new data
"""

# Calculate evaluation metrics
mae_2022 = mean_absolute_error(Y2, pred_2022)
rmse_2022 = sqrt(mean_squared_error(Y2, pred_2022))
r2_2022 = r2_score(Y2, pred_2022)

# Print the evaluation metrics
print("Mean Absolute Error (MAE):", mae_2022)
print("Root Mean Squared Error (RMSE):", rmse_2022)
print("R-squared (R^2):", r2_2022)

"""# **Deployment**




"""